{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4N-QZed-lsS"
      },
      "source": [
        "# CPSC 533R Visual AI - Assignment 2\n",
        "\n",
        "This Jupyter notebook provides downloads and defines a pytorch dataset of egocentric images and corresponding 2D pose, a pre-defined neural network, and plotting utility functions. We also provide training code for regressing 2D pose directly from the image. All modules should seamlessly integrate into your Assignment 1 solution as they use dictionaries for storing the input images and output labels. It is your task to extend this notebook with the tasks described in the Assignment2.pdf."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3kNUT74VzSM"
      },
      "source": [
        "# download dataset from the web (400 MB file from https://www.cs.ubc.ca/~rhodin/20_CPSC_532R_533R/assignments/EgoCap_nth10.hdf5)\n",
        "file_name = \"EgoCap_nth10.hdf5\"\n",
        "import os.path\n",
        "import urllib.request\n",
        "if not os.path.exists(file_name):\n",
        "    print(\"Downloading dataset, might take a while... its 400 MB\")\n",
        "    urllib.request.urlretrieve(\"https://www.cs.ubc.ca/~rhodin/20_CPSC_532R_533R/assignments/\"+file_name, file_name)\n",
        "    print(\"Done downloading\")\n",
        "else:\n",
        "    print(\"Dataset already present, nothing to be done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEw3FX9UWUxr"
      },
      "source": [
        "def dict_to_device(orig, device):\n",
        "    new = {}\n",
        "    for k,v in orig.items():\n",
        "        new[k] = v.to(device)\n",
        "    return new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw6VMbux55VA"
      },
      "source": [
        "# Definition the EgoCap dataset (small version)\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import h5py\n",
        "import os\n",
        "\n",
        "class EgoCapDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_folder):\n",
        "        super(EgoCapDataset).__init__();\n",
        "        data_file = 'EgoCap_nth10.hdf5'\n",
        "        print(\"Loading dataset to memory, can take some seconds\")\n",
        "        with h5py.File(data_file, 'r') as hf:\n",
        "            self.poses_2d = torch.from_numpy(hf['pose_2d'][...])\n",
        "            self.poses_3d = torch.from_numpy(hf['pose_3d'][...])\n",
        "            self.imgs  = torch.from_numpy(hf['img'][...])\n",
        "        print(\".. done loading\")\n",
        "        self.mean, self.std = torch.FloatTensor([0.485, 0.456, 0.406]), torch.FloatTensor([0.229, 0.224, 0.225])\n",
        "        self.normalize = transforms.Normalize(self.mean, self.std)\n",
        "        self.denormalize = transforms.Compose([transforms.Normalize(mean = [ 0., 0., 0. ], std = 1/self.std),\n",
        "                                               transforms.Normalize(mean = -self.mean, std = [ 1., 1., 1. ])])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.poses_2d.shape[0]\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        sample = {'img': self.normalize(self.imgs[idx].float()/255),\n",
        "                  'pose_2d': self.poses_2d[idx],\n",
        "                  'pose_3d': self.poses_3d[idx]}\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoC_DwKB55VJ"
      },
      "source": [
        "# skeleton pose definition\n",
        "# Labels are 2D (y, x) coordinate vectors, zero-based starting from the top-left pixel. They appear in the following order: \n",
        "joint_names = ['head', 'neck', 'left-shoulder', 'left-elbow', 'left-wrist', 'left-finger', 'right-shoulder', 'right-elbow', 'right-wrist', 'right-finger', 'left-hip', 'left-knee', 'left-ankle', 'left-toe', 'right-hip', 'right-knee', 'right-ankle', 'right-toe']\n",
        "# the skeleton is defined as a set of bones (pairs of skeleton joint indices):\n",
        "bones_ego_str = [('head', 'neck'), ('neck', 'left-shoulder'), ('left-shoulder', 'left-elbow'), ('left-elbow', 'left-wrist'), ('left-wrist', 'left-finger'), ('neck', 'right-shoulder'), ('right-shoulder', 'right-elbow'), ('right-elbow', 'right-wrist'), ('right-wrist', 'right-finger'), \n",
        "                 ('left-shoulder', 'left-hip'), ('left-hip', 'left-knee'), ('left-knee', 'left-ankle'), ('left-ankle', 'left-toe'), ('right-shoulder', 'right-hip'), ('right-hip', 'right-knee'), ('right-knee', 'right-ankle'), ('right-ankle', 'right-toe'), ('right-shoulder', 'left-shoulder'), ('right-hip', 'left-hip')]\n",
        "bones_ego_idx = [(joint_names.index(b[0]),joint_names.index(b[1])) for b in bones_ego_str]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khpc584q55VO"
      },
      "source": [
        "# plotting utility functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "r\"\"\"Plots skeleton pose on a matplotlib axis.\n",
        "\n",
        "        Args:\n",
        "            ax (Axis): plt axis to plot\n",
        "            pose_2d (FloatTensor): tensor of keypoints, of shape K x 2\n",
        "            bones (list): list of tuples, each tuple defining the keypoint indices to be connected by a bone \n",
        "        Returns:\n",
        "            Module: self\n",
        "\"\"\"\n",
        "def plot_skeleton(ax, pose_2d, bones=bones_ego_idx, linewidth=2, linestyle='-', label=None):\n",
        "    cmap = plt.get_cmap('hsv')\n",
        "    for i, bone in enumerate(bones):\n",
        "        color = cmap(bone[1] * cmap.N // len(joint_names)) # color according to second joint index\n",
        "        if i!=0:\n",
        "            label=None\n",
        "        ax.plot(pose_2d[bone,0], pose_2d[bone,1], linestyle, color=color, linewidth=linewidth, label=label)\n",
        "\n",
        "r\"\"\"Plots list of skeleton poses and image.\n",
        "\n",
        "        Args:\n",
        "            poses (list): list of pose tensors to be plotted\n",
        "            ax (Axis): plt axis to plot\n",
        "            bones (list): list of tuples, each tuple defining the keypoint indices to be connected by a bone \n",
        "        Returns:\n",
        "            Module: self\n",
        "\"\"\"\n",
        "def plotPosesOnImage(poses, img, ax=plt, labels=None):\n",
        "    img_pil = torchvision.transforms.ToPILImage()(img)\n",
        "    img_size = torch.FloatTensor(img_pil.size)\n",
        "    linestyles = ['-', '--', '-.', ':']\n",
        "    for i, p in enumerate(poses):\n",
        "        pose_px = p*img_size\n",
        "        plot_skeleton(ax, pose_px, linestyle=linestyles[i%len(linestyles)], label=labels[i])\n",
        "    ax.imshow(img_pil)\n",
        "\n",
        "r\"\"\"Converts a multi channel heatmap to an RGB color representation for display.\n",
        "\n",
        "        Args:\n",
        "            heatmap (tensor): of size C X H x W\n",
        "        Returns:\n",
        "            image (tensor): of size 3 X H x W\n",
        "\"\"\"\n",
        "def heatmap2image(heatmap):\n",
        "    C,H,W = heatmap.shape\n",
        "    cmap = plt.get_cmap('hsv')\n",
        "    img = torch.zeros(3,H,W).to(heatmap.device)\n",
        "    for i in range(C):\n",
        "        color = torch.FloatTensor(cmap(i * cmap.N // C)[:3]).reshape([-1,1,1]).to(heatmap.device)\n",
        "        img = torch.max(img, color * heatmap[i]) # max in case of overlapping position of joints\n",
        "    # heatmap and probability maps might have small maximum value. Normalize per channel to make each of them visible\n",
        "    img_max, indices = torch.max(img,dim=-1,keepdim=True)\n",
        "    img_max, indices = torch.max(img_max,dim=-2,keepdim=True)\n",
        "    return img/img_max"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaOGZWro55VS"
      },
      "source": [
        "# setting up the dataset and train/val splits\n",
        "path='./'\n",
        "ecds = EgoCapDataset(data_folder=path)\n",
        "\n",
        "val_ratio = 0.2\n",
        "val_size = int(len(ecds)*val_ratio)\n",
        "indices_val = list(range(0, val_size))\n",
        "indices_train = list(range(val_size, len(ecds)))\n",
        "\n",
        "val_set   = torch.utils.data.Subset(ecds, indices_val)\n",
        "train_set = torch.utils.data.Subset(ecds, indices_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oWc4_hlBz0A"
      },
      "source": [
        "# playing with data and plotting functions\n",
        "sample_train = train_set[100]\n",
        "sample_val = val_set[100]\n",
        "plotPosesOnImage([sample_train['pose_2d']], ecds.denormalize(sample_train['img']), labels=['training example'])\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plotPosesOnImage([sample_val['pose_2d']], ecds.denormalize(sample_val['img']), labels=['validation example'])\n",
        "plt.legend()\n",
        "plt.show()\n",
        "print('dataset length', len(ecds))\n",
        "print('train_set length', len(train_set))\n",
        "print('val_set length', len(val_set))\n",
        "print('pose shape',sample_train['pose_2d'].shape)\n",
        "print('img shape',sample_train['img'].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATLxVpnr55VY"
      },
      "source": [
        "# define the dataset loader (batch size, shuffling, ...)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size = 16, num_workers=0, pin_memory=False, shuffle=True, drop_last=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size = 16, num_workers=0, pin_memory=False, shuffle=False, drop_last=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVXkgRUMpZ_2"
      },
      "source": [
        "## Regression-based pose inference\n",
        "\n",
        "We provide a baseline method that regresses 2D pose straight from the image. Make sure that it runs on your hardware and configuration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JQWhWsn55Vd"
      },
      "source": [
        "# define a regression network that works on dictionaries\n",
        "class RegressionNet(torch.nn.Module):\n",
        "    def __init__(self, num_joints):\n",
        "        super().__init__()\n",
        "        self.num_joints = num_joints\n",
        "        self.net = torchvision.models.resnet50(num_classes=num_joints*2)\n",
        "\n",
        "    def forward(self, dictionary):\n",
        "        return {'pose_2d' : self.net(dictionary['img']).reshape(-1,self.num_joints,2)}\n",
        "num_joints = len(joint_names)\n",
        "regression_network = RegressionNet(num_joints=num_joints).cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ438TMqqx-h"
      },
      "source": [
        "# training loop for regression\n",
        "%matplotlib inline\n",
        "from IPython import display\n",
        "optimizer = torch.optim.Adam(regression_network.parameters(), lr=0.001)\n",
        "fig = plt.figure(figsize=(20, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
        "axes = fig.subplots(1,2)\n",
        "num_epochs = 2\n",
        "losses = []\n",
        "for e in range(num_epochs):\n",
        "    train_iter = iter(train_loader)\n",
        "    regression_network.train()\n",
        "    for i in range(len(train_loader)):\n",
        "        batch_cpu = next(train_iter)\n",
        "        batch_gpu = dict_to_device(batch_cpu, 'cuda')\n",
        "        pred = regression_network(batch_gpu)\n",
        "        pred_cpu = dict_to_device(pred, 'cpu')\n",
        "\n",
        "        #print(pred['pose_2d'], batch_gpu['pose_2d'])\n",
        "        loss = torch.nn.functional.mse_loss(pred['pose_2d'], batch_gpu['pose_2d'])\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        if i%10==0:\n",
        "            # clear figures for a new update\n",
        "            for ax in axes:\n",
        "                ax.cla()\n",
        "            # plot the predicted pose and ground truth pose on the image\n",
        "            plotPosesOnImage([pred_cpu['pose_2d'][0].detach(), \n",
        "                           batch_cpu['pose_2d'][0]], \n",
        "                          ecds.denormalize(batch_cpu['img'][0]), ax=axes[0], labels=['prediction','ground truth label'])\n",
        "            axes[0].legend()\n",
        "            axes[0].set_title('Input image with predicted pose (solid) and GT pose (dashed)')\n",
        "\n",
        "            # plot the training error on a log plot\n",
        "            axes[1].plot(losses, label='loss')\n",
        "            axes[1].set_yscale('log')\n",
        "            axes[1].set_title('Training loss')\n",
        "            axes[1].set_xlabel('number of gradient iterations')\n",
        "            axes[1].legend()\n",
        "\n",
        "            # clear output window and diplay updated figure\n",
        "            display.clear_output(wait=True)\n",
        "            display.display(plt.gcf())\n",
        "            print(\"Epoch {}, iteration {} of {} ({} %), loss={}\".format(e, i, len(train_loader), 100*i//len(train_loader), losses[-1]))\n",
        "            print(\"Training for the specified amount of epochs would take long.\\nStop the process once you verified that the training works on your setup.\")\n",
        "plt.close('all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll9Ns_gr55Vr"
      },
      "source": [
        "## Heatmap-based pose classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5kiahQf55Vt"
      },
      "source": [
        "# Detection network that handles dictionaries as input and output\n",
        "class HeatNetWrapper(torch.nn.Module):\n",
        "    def __init__(self, net):\n",
        "        super().__init__()\n",
        "        self.net = net\n",
        "\n",
        "    def forward(self, dictionary):\n",
        "        return {'heatmap':(self.net(dictionary['img'])['out'])}\n",
        "num_joints = len(joint_names)\n",
        "det_network = HeatNetWrapper(torchvision.models.segmentation.deeplabv3_resnet50(num_classes=num_joints)).cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZcaiyRHDC6C"
      },
      "source": [
        "# Function that takes an NxKx2 pose vector (N: batch dimension, K: number of keypoints) to create stacks of heatmaps that have Gaussian distribution with the mean at the keypoint. \n",
        "# Determine a suitable standard deviation by visually inspecting the result and monitoring the training. Both, a too big and too small Gaussian size has drawbacks.\n",
        "# The second argument specifies the output dimensions of the map. Note that the keypoints are defined in normalized coordinates, ranging from 0..1 irrespectively of the image resolution.\n",
        "import math\n",
        "\n",
        "r\"\"\"Creates a heatmap stack, with each channel having Gaussian form with mean at the pose keypoint locations\n",
        "\n",
        "        Args:\n",
        "            pose_2d (tensor): tensor of size N x K x 2, with K the number of keypoints. Keypoint locations store relative keypoint locations, i.e. both x and y coordinated in the range 0..1\n",
        "            map_size (tuple): height and width of the heatmap to be generated\n",
        "        Returns:\n",
        "            heatmap (tensor): tensor of size N x K x H x W, with K the number of keypoints\n",
        "\"\"\"\n",
        "def pose2heatmap(pose_2d, map_size):\n",
        "\n",
        "    # TODO: Task I\n",
        "    sigma_x = 0.05\n",
        "    sigma_y = sigma_x * map_size[1] / map_size[0]\n",
        "    z = 1/(2 * math.pi * sigma_x * sigma_y)\n",
        "    N = pose_2d.shape[0]\n",
        "    \n",
        "    x_list = torch.linspace(0, 1, map_size[1]).to(pose_2d.device)\n",
        "    y_list = torch.linspace(0, 1, map_size[0]).to(pose_2d.device)\n",
        "    mu_x = pose_2d[:, :, 0].reshape(N, -1, 1)\n",
        "    mu_y = pose_2d[:, :, 1].reshape(N, -1, 1)\n",
        "    \n",
        "    diff_x = ((x_list-mu_x)**2).unsqueeze(-2) / (2*sigma_x**2)\n",
        "    diff_y = ((y_list-mu_y)**2).unsqueeze(-1) / (2*sigma_y**2)\n",
        "    heatmap = z * torch.exp(-(diff_x + diff_y))\n",
        "\n",
        "    return heatmap\n",
        "\n",
        "r\"\"\"Takes a heatmap and returns the location of the maximum value in the heatmap\n",
        "\n",
        "        Args:\n",
        "            heatmap (tensor): tensor of size N x K x H x W, with K the number of keypoints\n",
        "        Returns:\n",
        "            pose (tensor): tensor of size N x K x 2, the 2D pose for each image in the batch\n",
        "\"\"\"       \n",
        "def heatmap2pose(heatmap):\n",
        "    max_alongx, _ = torch.max(heatmap, dim=-1)\n",
        "    max_alongy, _ = torch.max(heatmap, dim=-2)\n",
        "    _, max_y_index= torch.max(max_alongx, dim=-1)\n",
        "    _, max_x_index = torch.max(max_alongy, dim=-1)\n",
        "    res_y, res_x = heatmap.shape[-2:]\n",
        "    return torch.stack([max_x_index/float(res_x), max_y_index/float(res_y)],dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RT0BaEh7E90z"
      },
      "source": [
        "# training loop for heatmap prediction\n",
        "%matplotlib inline\n",
        "from IPython import display\n",
        "optimizer = torch.optim.Adam(det_network.parameters(), lr=0.003)\n",
        "fig=plt.figure(figsize=(20, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
        "axes=fig.subplots(1,4)\n",
        "losses = []\n",
        "num_epochs = 2\n",
        "for e in range(num_epochs):\n",
        "    train_iter = iter(train_loader)\n",
        "    for i in range(len(train_loader)):\n",
        "        batch_cpu = next(train_iter)\n",
        "        batch_gpu = dict_to_device(batch_cpu, 'cuda')\n",
        "        pred_gpu = det_network(batch_gpu)\n",
        "        pred_cpu = dict_to_device(pred_gpu, 'cpu')\n",
        "\n",
        "        # convert between representations\n",
        "        img_shape = batch_gpu['img'].shape\n",
        "        gt_heatmap_gpu = pose2heatmap(batch_gpu['pose_2d'], img_shape[-2:])\n",
        "        pred_pose = heatmap2pose(pred_cpu['heatmap']).cpu() # Note, not differentiable\n",
        "        gt_pose_max = heatmap2pose(gt_heatmap_gpu).cpu()\n",
        "\n",
        "        # optimize network\n",
        "        loss = torch.nn.functional.mse_loss(pred_gpu['heatmap'], gt_heatmap_gpu)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # display progress\n",
        "        if i%10==0:\n",
        "            # clear figure for a new update\n",
        "            for ax in axes: \n",
        "                ax.cla()\n",
        "            # plot the ground truth and the predicted pose on top of the image\n",
        "            plotPosesOnImage([pred_pose[0], batch_cpu['pose_2d'][0]], ecds.denormalize(batch_cpu['img'][0]), ax=axes[0], labels=['prediction','ground truth label'])\n",
        "            axes[0].set_title('Input image with predicted and GT pose')\n",
        "            axes[0].legend()\n",
        "\n",
        "            # plot the predicted heatmap map and the predicted pose on top\n",
        "            plotPosesOnImage([pred_pose[0]], heatmap2image(pred_cpu['heatmap'][0]), ax=axes[1], labels=['prediction'])\n",
        "            axes[1].set_title('Predicted heatmap with pose overlayed')\n",
        "            axes[1].legend()\n",
        "\n",
        "            # plot the reference heatmap map and the GT pose on top\n",
        "            plotPosesOnImage([gt_pose_max[0]], heatmap2image(gt_heatmap_gpu[0].cpu()), ax=axes[2], labels=['label'])\n",
        "            axes[2].set_title('Reference heatmap with pose inferred from maxima')\n",
        "            axes[2].legend()\n",
        "\n",
        "            # plot the current training error on a logplot\n",
        "            axes[3].plot(losses, label='loss'); axes[3].set_yscale('log')\n",
        "            axes[3].set_title('Training error in log scale')\n",
        "            axes[3].legend()\n",
        "\n",
        "            # clear output window and diplay updated figure\n",
        "            display.clear_output(wait=True)\n",
        "            display.display(plt.gcf())\n",
        "            print(\"Epoch {}, iteration {} of {} ({} %), loss={}\".format(e, i, len(train_loader), 100*i//len(train_loader), losses[-1]))\n",
        "            print(\"Training for the specified amount of epochs would take long.\\nStop the process once you verified that your method works.\")\n",
        "plt.close('all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42K7hvKNCvxg"
      },
      "source": [
        "## Integral pose regression via heatmaps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxHtRr8E55V3"
      },
      "source": [
        "def integral_heatmap_layer(dict):\n",
        "    # compute coordinate matrix\n",
        "    heatmap = dict['heatmap']\n",
        "\n",
        "    # TODO: Task II\n",
        "    n, c, h, w = heatmap.shape\n",
        "\n",
        "    z = torch.sum(torch.exp(heatmap), (2, 3)).view(n, c, 1, 1)\n",
        "    h_norm = torch.exp(heatmap) / z\n",
        "\n",
        "    y_vals = torch.linspace(0, 1, h).to(heatmap.device).unsqueeze(-1)\n",
        "    x_vals = torch.linspace(0, 1, w).to(heatmap.device).unsqueeze(-2)\n",
        "    y_means = torch.sum(h_norm * y_vals, dim = (2, 3))\n",
        "    x_means = torch.sum(h_norm * x_vals, dim = (2, 3))\n",
        "\n",
        "    pose = torch.stack((x_means, y_means), -1)\n",
        "\n",
        "    return {'probabilitymap': h_norm, 'pose_2d': pose}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcBoyjlAC9Yx"
      },
      "source": [
        "int_network = HeatNetWrapper(torchvision.models.segmentation.deeplabv3_resnet50(num_classes=num_joints)).cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPbZXEE455V9"
      },
      "source": [
        "%matplotlib inline\n",
        "from IPython import display\n",
        "import time\n",
        "optimizer = torch.optim.Adam(int_network.parameters(), lr=0.001)\n",
        "fig=plt.figure(figsize=(20, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
        "axes=fig.subplots(1,3)\n",
        "losses = []\n",
        "num_epochs = 100\n",
        "for e in range(num_epochs):\n",
        "    train_iter = iter(train_loader)\n",
        "    for i in range(len(train_loader)):\n",
        "        batch_cpu = next(train_iter)\n",
        "        batch_gpu = dict_to_device(batch_cpu, 'cuda')\n",
        "        pred_raw = int_network(batch_gpu)\n",
        "        pred_integral = integral_heatmap_layer(pred_raw) # note, this function must be differentiable\n",
        "\n",
        "        # optimize network\n",
        "        loss = torch.nn.functional.mse_loss(pred_integral['pose_2d'], batch_gpu['pose_2d'])\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # plot progress\n",
        "        if i%10==0:\n",
        "            # clear figures for a new update\n",
        "            for ax in axes:\n",
        "                ax.cla()\n",
        "            pred_cpu = dict_to_device(pred_integral, 'cpu')\n",
        "            # plot the ground truth and the predicted pose on top of the image\n",
        "            plotPosesOnImage([pred_cpu['pose_2d'][0].detach(), batch_cpu['pose_2d'][0]], ecds.denormalize(batch_cpu['img'][0]), ax=axes[0], labels=['prediction', 'ground truth label'])\n",
        "            axes[0].set_title('Input image with predicted pose (solid) and GT pose (dashed)')\n",
        "            axes[0].legend()\n",
        "\n",
        "            # plot the predicted probability map and the predicted pose on top\n",
        "            plotPosesOnImage([pred_cpu['pose_2d'][0].detach()], heatmap2image(pred_cpu['probabilitymap'][0]).detach(), ax=axes[1], labels=['prediction'])\n",
        "            axes[1].set_title('Predicted probability map with predicted pose overlayed')\n",
        "            axes[1].legend()\n",
        "\n",
        "            # plot the current training error on a logplot\n",
        "            axes[2].plot(losses)\n",
        "            axes[2].set_yscale('log')\n",
        "            axes[2].set_title('Training error in log scale')\n",
        "            axes[2].legend()\n",
        "\n",
        "            # clear output window and diplay updated figure\n",
        "            display.clear_output(wait=True)\n",
        "            display.display(plt.gcf())\n",
        "            print(\"Epoch {}, iteration {} of {} ({} %), loss={}\".format(e, i, len(train_loader), 100*i//len(train_loader), losses[-1]))\n",
        "            print(\"Training for the specified amount of epochs would take long.\\nStop the process once you verified that your method works.\")\n",
        "plt.close('all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkeghnvS55WC"
      },
      "source": [
        "# TODO: Task III, validation, which approach is the best?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NMnFI4cRDcv"
      },
      "source": [
        "### Bonus Task: Can you implement a denisty network version of integral pose regression by estimating the std/sigma from the heatmap and using it for the probabilitic regression loss?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQsYnG1oQ378"
      },
      "source": [
        "# implement the bonus task here, with leaving the other code blocks unchanged for grading"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}